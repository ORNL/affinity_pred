#!/usr/bin/env bash
#BSUB -P STF006
#BSUB -W 2:00
#BSUB -q batch
#BSUB -nnodes 1600
#BSUB -J finetune_ensemble_regex
#BSUB -o finetune_ensemble_regex.o%J
#BSUB -e finetune_ensemble_regex.e%J

module load cuda/11.0.3
module load open-ce/1.2.0-py38-0
conda activate /gpfs/alpine/world-shared/bip214/deepspeed-env

export CUDA_HOME=${CUDA_TOOLKIT_ROOT_DIR}

export HF_HOME=/gpfs/alpine/world-shared/bip214/affinity_pred/train
export HF_DATASETS_CACHE=/gpfs/alpine/world-shared/bip214/affinity_pred/train/dataset-cache
export TORCH_EXTENSIONS_DIR=/gpfs/alpine/world-shared/bip214/affinity_pred/train/build

# undo some conda env variables
export CC=`which gcc`
export GCC=`which gcc`
export CXX=`which g++`

export OMP_NUM_THREADS=1
export PYTHONUNBUFFERED=1

# clear stale lock files
rm -f `find -name *lock`

for ENSEMBLE_ID in `seq 1 5`; do
    jsrun -n 320 -g 6 -a 6 -c 42 python ../affinity_pred/finetune.py \
        --deepspeed='ds_config_rocm.json'\
        --model_type='regex' \
        --n_cross_attention=10\
        --dataset='/gpfs/alpine/world-shared/bip214/binding_affinity'\
        --split='train'\
        --output_dir=./results_ensemble_regex_10cross_${ENSEMBLE_ID}\
        --num_train_epochs=45\
        --per_device_train_batch_size=32\
        --per_device_eval_batch_size=32\
        --warmup_steps=5\
        --learning_rate=2.5e-4\
        --weight_decay=0.01\
        --logging_dir=./logs_ensemble_regex_10cross_${ENSEMBLE_ID}\
        --logging_steps=1\
        --evaluation_strategy="epoch"\
        --gradient_accumulation_steps=1\
        --fp16=True\
        --run_name="seq_smiles_affinity"\
        --save_strategy="epoch"\
        --seed=$((42+${ENSEMBLE_ID})) &
done

wait
