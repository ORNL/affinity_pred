#!/usr/bin/env bash
#BSUB -P STF006
#BSUB -W 12:00
#BSUB -q batch
#BSUB -nnodes 768
#BSUB -J finetune[1-5]
#BSUB -o finetune.o%J
#BSUB -e finetune.e%J

source activate /gpfs/alpine/world-shared/bip214/opence-env

export HF_HOME=/gpfs/alpine/world-shared/bip214/affinity_pred/train
export TORCH_EXTENSIONS_DIR=/gpfs/alpine/world-shared/bip214/affinity_pred/train/build

module load cuda/10.2

# undo some conda env variables
export CC=`which gcc`
export GCC=`which gcc`
export CXX=`which g++`

# open files limit (for more than ~1000 nodes)
ulimit -n 14000
export OMP_NUM_THREADS=1

# clear stale lock files
rm -f `find -name *lock`

# the id of this job in the ensemble model
export ENSEMBLE_ID=${LSB_JOBINDEX}

jsrun $BAD_HOSTS -r 1 -g 6 -a 6 -c 42 python ../affinity_pred/finetune.py \
    --deepspeed='ds_config_stage2.json'\
    --output_dir='./results_ensemble_${ENSEMBLE_ID}'\
    --num_train_epochs=45\
    --per_device_train_batch_size=16\
    --per_device_eval_batch_size=16\
    --warmup_steps=5\
    --learning_rate=3e-05\
    --weight_decay=3e-7\
    --logging_dir='./logs_ensemble_${ENSEMBLE_ID}'\
    --logging_steps=1\
    --evaluation_strategy="epoch"\
    --gradient_accumulation_steps=1\
    --fp16=True\
    --run_name="seq_smiles_affinity"\
    --save_strategy="epoch"\
    --seed=$((42+${ENSEMBLE_ID}))
