#!/usr/bin/env bash
#BSUB -P STF006
#BSUB -W 6:00
#BSUB -q batch
#BSUB -nnodes 768
#BSUB -J infer
#BSUB -o infer.o%J
#BSUB -e infer.e%J

source activate /gpfs/alpine/world-shared/bip214/opence-env

module load cuda/10.2

# open files limit (for more than ~1000 nodes)
ulimit -n 14000
export OMP_NUM_THREADS=1

export NUM_NODES=`cat ${LSB_DJOB_HOSTFILE[0]} | sort | uniq | grep -v login | grep -v batch | wc -l`
export NUM_WORKER_NODES=$((${NUM_NODES}-1))
export NUM_WORKERS=$((6*${NUM_WORKER_NODES}))

jsrun -n1 -g 1 -a1 -c 42 dask-scheduler --interface ib0 --scheduler-file my-scheduler.json &
jsrun -n ${NUM_WORKERS} -a 1 -g 1 -c 7 -b rs dask-worker --scheduler-file my-scheduler.json --death-timeout 60 --interface ib0 --nthreads 1 --memory-limit 160GB &

sleep 120
python ../affinity_pred/infer.py --per_device_eval_batch_size=32 --output_dir='results'

